{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9KiyXRZYfLr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install -U sentence-transformers umap-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook was run in Google Colab and used to explore different embeddings and dimensionality reduction strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MVhd3K2HY-rb"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from umap import UMAP\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lrnwswxblUY4"
      },
      "outputs": [],
      "source": [
        "df = pd.read_pickle(\"drive/MyDrive/Thesis-CDTM-Backup/data/predictions/df_chunks_embedding.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "dw_cUaJzZson"
      },
      "outputs": [],
      "source": [
        "df = pd.read_pickle(\"drive/MyDrive/Thesis-CDTM-Backup/data/predictions/df_chunks_emb.pkl\")\n",
        "df = df.rename(columns={\"chunks\": \"text\"}).drop(columns=\"embeddings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "EZZURdG1ddzc"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer('BAAI/bge-large-en', device='cuda')\n",
        "instruction = \"Represent this sentence for searching relevant passages: \"\n",
        "queries = [instruction + t for t in df.text.tolist()]\n",
        "embeddings = model.encode(queries, normalize_embeddings=True, show_progress_bar=True)\n",
        "df[\"emb_bge_large_en\"] = embeddings.tolist()\n",
        "pca_model = PCA()\n",
        "pca_model.fit(embeddings)\n",
        "index_80 = np.min(np.argwhere(np.cumsum(pca_model.explained_variance_ratio_)> 0.8))\n",
        "print(index_80, \"Principal componenets needed to explain 80% of variance\")\n",
        "pca_model = PCA(n_components=index_80)\n",
        "pca_embeddings = pca_model.fit_transform(embeddings)\n",
        "df[\"emb_bge_large_en_pca_80\"] = pca_embeddings.tolist()\n",
        "pca_model = PCA(n_components=50)\n",
        "pca_embeddings = pca_model.fit_transform(embeddings)\n",
        "df[\"emb_bge_large_en_pca_50\"] = pca_embeddings.tolist()\n",
        "pca_model = PCA(n_components=10)\n",
        "pca_embeddings = pca_model.fit_transform(embeddings)\n",
        "df[\"emb_bge_large_en_pca_10\"] = pca_embeddings.tolist()\n",
        "for n_neighbors in [5, 10, 15]:\n",
        "  for n_components in [5, 15, 30, 50]:\n",
        "    umap_model = UMAP(n_neighbors=n_neighbors, n_components=n_components, min_dist=0.0, metric='cosine')\n",
        "    embeddings_umap = umap_model.fit_transform(embeddings)\n",
        "    df[f\"emb_bge_large_en_umap_{n_neighbors}_{n_components}\"] = embeddings_umap.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "B_9pstr7e80D"
      },
      "outputs": [],
      "source": [
        "df.to_pickle(\"drive/MyDrive/Thesis-CDTM-Backup/data/predictions/df_chunks_embedding.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "6PvdhSTRacCD"
      },
      "outputs": [],
      "source": [
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhEPLb0FeFzf"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\"intfloat/e5-large-v2\", device='cuda')\n",
        "instruction = \"query: \"\n",
        "queries = [instruction + t for t in df.text.tolist()]\n",
        "embeddings = model.encode(queries, normalize_embeddings=True, show_progress_bar=True)\n",
        "df[\"emb_e5_large_v2\"] = embeddings.tolist()\n",
        "pca_model = PCA()\n",
        "pca_model.fit(embeddings)\n",
        "index_80 = np.min(np.argwhere(np.cumsum(pca_model.explained_variance_ratio_)> 0.8))\n",
        "print(index_80, \"Principal componenets needed to explain 80% of variance\")\n",
        "pca_model = PCA(n_components=index_80)\n",
        "pca_embeddings = pca_model.fit_transform(embeddings)\n",
        "df[\"emb_e5_large_v2_pca_80\"] = pca_embeddings.tolist()\n",
        "pca_model = PCA(n_components=50)\n",
        "pca_embeddings = pca_model.fit_transform(embeddings)\n",
        "df[\"emb_e5_large_v2_pca_50\"] = pca_embeddings.tolist()\n",
        "pca_model = PCA(n_components=10)\n",
        "pca_embeddings = pca_model.fit_transform(embeddings)\n",
        "df[\"emb_e5_large_v2_pca_10\"] = pca_embeddings.tolist()\n",
        "for n_neighbors in [5, 10, 15]:\n",
        "  for n_components in [5, 15, 30, 50]:\n",
        "    umap_model = UMAP(n_neighbors=n_neighbors, n_components=n_components, min_dist=0.0, metric='cosine')\n",
        "    embeddings_umap = umap_model.fit_transform(embeddings)\n",
        "    df[f\"emb_e5_large_v2_umap_{n_neighbors}_{n_components}\"] = embeddings_umap.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "k2h4CITifgLH"
      },
      "outputs": [],
      "source": [
        "df.to_pickle(\"drive/MyDrive/Thesis-CDTM-Backup/data/predictions/df_chunks_embedding.pkl\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
