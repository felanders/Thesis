{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "import os\n",
    "import textwrap\n",
    "import datasets\n",
    "from dotenv import dotenv_values\n",
    "from pathlib import Path\n",
    "from scipy.special import softmax\n",
    "from scipy.stats import entropy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(19950808)\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "config = dotenv_values(\"./../../../config/.env\") # take environment variables from .env.\n",
    "base_path = Path(config[\"BASE_PATH\"])\n",
    "sys.path.append(str(base_path/\"code\"))\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import cohere\n",
    "import backoff\n",
    "from bertopic.representation import Cohere\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.vectorizers import ClassTfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_name = \"emb_bge_large_en\"\n",
    "n_neighbors = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(base_path/\"data/predictions/df_chunks_embedding.pkl\")\n",
    "df.report_id = df.report_id.astype(str)\n",
    "df.filing_type = df.filing_type.apply(lambda x: \"10-K\" if x[:4] == \"10-K\" else \".pdf\")\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df_risks = pd.read_pickle(base_path/'data/processed/df_risk_descriptions.pkl')\n",
    "df_risks = df_risks[~df_risks.Source.str.contains(\"internal\")]\n",
    "df_risks.reset_index(drop=True, inplace=True)\n",
    "\n",
    "embeddings = np.array([np.array(x) for x in df[embedding_name].tolist()])\n",
    "embeddings_2d = UMAP(n_neighbors=n_neighbors, n_components=2, min_dist=0.0, metric='cosine', random_state=19950808).fit_transform(embeddings)\n",
    "\n",
    "df[\"x\"] = embeddings_2d[:,0].tolist()\n",
    "df[\"y\"] = embeddings_2d[:,1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_to_indices(source_df, documents):\n",
    "    return [source_df[source_df.text == document].index[0] for document in documents]\n",
    "\n",
    "def get_top_k_matches(indices, semantic_scores, top_k=10):\n",
    "    df = pd.DataFrame()\n",
    "    for score_list in [semantic_scores[index] for index in indices]:\n",
    "        df = df.append(pd.DataFrame(score_list), ignore_index=True)\n",
    "    df.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "    df.drop_duplicates(subset=['corpus_id'], inplace=True, keep=\"first\")\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df.head(top_k)\n",
    "\n",
    "def map_topics_risks_and_paragraphs(df, tm_df, df_risks, top_k, embedding_name):\n",
    "    ## Map topics to risks\n",
    "    embeddings = np.array(df[embedding_name].tolist())\n",
    "    risk_embeddings = np.array(df_risks[embedding_name].tolist())\n",
    "    \n",
    "    semantic_scores = util.semantic_search(\n",
    "        embeddings,\n",
    "        risk_embeddings,\n",
    "        top_k=top_k,\n",
    "        score_function=util.cos_sim)\n",
    "    all_scores = util.cos_sim(embeddings, risk_embeddings).flatten()\n",
    "\n",
    "    mean = float(all_scores.mean())\n",
    "    std = float(all_scores.std())\n",
    "\n",
    "    topics_to_risks_df = pd.DataFrame()\n",
    "    for i, row in tm_df.iterrows():\n",
    "        indices = docs_to_indices(source_df=df, documents=row[\"Representative_Docs\"])\n",
    "        matches = get_top_k_matches(indices=indices, semantic_scores=semantic_scores, top_k=top_k)\n",
    "        matches.rename(columns = {'corpus_id': \"risks_index\"}, inplace = True)\n",
    "        matches[\"Topic\"] = i-1\n",
    "        matches.score = matches.score.sub(mean).div(std)\n",
    "        topics_to_risks_df = topics_to_risks_df.append(matches[matches.score > 0], ignore_index=True)\n",
    "    \n",
    "    topics_to_risks_df.rename(columns = {'score': \"Score\"}, inplace = True)\n",
    "\n",
    "    ## Map Risks to topics\n",
    "    semantic_scores = util.semantic_search(\n",
    "        risk_embeddings,\n",
    "        embeddings,\n",
    "        top_k=top_k,\n",
    "        score_function=util.cos_sim)\n",
    "\n",
    "    risks_to_topics_df = pd.DataFrame()\n",
    "    for i, row in df_risks.iterrows():\n",
    "        indices = [i]\n",
    "        matches = get_top_k_matches(indices=indices, semantic_scores=semantic_scores, top_k=top_k)\n",
    "        matches.rename(columns = {'corpus_id': \"topics_index\"}, inplace = True)\n",
    "        matches[\"risk_index\"] = i\n",
    "        matches.score = matches.score.sub(mean).div(std)\n",
    "        risks_to_topics_df = risks_to_topics_df.append(matches[matches.score > 0], ignore_index=True)\n",
    "\n",
    "    risks_to_topics_df.rename(columns = {'score': \"Score\"}, inplace = True)\n",
    "\n",
    "    ## Map Risks to paragraphs\n",
    "    semantic_scores = util.semantic_search(\n",
    "        embeddings,\n",
    "        risk_embeddings,\n",
    "        top_k=top_k,\n",
    "        score_function=util.cos_sim)\n",
    "\n",
    "    risks_to_paragraphs_df = pd.DataFrame()\n",
    "    for i, row in df.iterrows():\n",
    "        indices = [i]\n",
    "        matches = get_top_k_matches(indices=indices, semantic_scores=semantic_scores, top_k=top_k)\n",
    "        matches.rename(columns = {'corpus_id': \"paragraphs_index\"}, inplace = True)\n",
    "        matches[\"risk_index\"] = i\n",
    "        matches.score = matches.score.sub(mean).div(std)\n",
    "        risks_to_paragraphs_df = risks_to_paragraphs_df.append(matches[matches.score > 0], ignore_index=True)\n",
    "\n",
    "    risks_to_paragraphs_df.rename(columns = {'score': \"Score\"}, inplace = True)\n",
    "\n",
    "    result_dict = {\n",
    "        \"topics_to_risks\": topics_to_risks_df, \n",
    "        \"risks_to_topics\": risks_to_topics_df,\n",
    "        \"risks_to_paragraphs\":  risks_to_paragraphs_df\n",
    "    }\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year_counts(topic, year_counts_df):\n",
    "    year_count_dict = {}\n",
    "    dff =  year_counts_df[year_counts_df.Topic == topic]\n",
    "    for year in range(dff.Year.min(), dff.Year.max()+1):\n",
    "        values = dff[dff.Year == year].Count.values\n",
    "        if values.size >0:\n",
    "            year_count_dict[year] = values[0]\n",
    "        else:\n",
    "            year_count_dict[year] = 0\n",
    "    return year_count_dict\n",
    "\n",
    "def get_max_count_year(year_count_dict):\n",
    "    return list(year_count_dict.keys())[np.array(list(year_count_dict.values())).argmax()]\n",
    "\n",
    "def get_first_year(year_count_dict):\n",
    "    return list(year_count_dict.keys())[0]\n",
    "\n",
    "def get_last_year(year_count_dict):\n",
    "    return list(year_count_dict.keys())[-1]\n",
    "\n",
    "def get_entropy(year_count_dict):\n",
    "    counts = np.array(list(year_count_dict.values()))\n",
    "    counts = counts/counts.sum()\n",
    "    return entropy(counts)\n",
    "\n",
    "def create_and_save_topic_model_and_sematic_similarity(df, embedding_name,  n_neigbors, df_risks, top_k):\n",
    "    embeddings = np.array([np.array(x) for x in df[embedding_name].tolist()])\n",
    "    docs = df.text.tolist()\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 2))\n",
    "    ctfidf_model = ClassTfidfTransformer()\n",
    "    embedding_model = SentenceTransformer(\"BAAI/bge-large-en\")\n",
    "    umap_model = UMAP(n_neighbors=n_neigbors, n_components=5, min_dist=0.0, metric='cosine', random_state=19950808)\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=n_neigbors, metric='euclidean', cluster_selection_method=\"eom\", cluster_selection_epsilon=0.2, prediction_data=True)\n",
    "    co = cohere.Client(config[\"COHERE_API_KEY\"])\n",
    "    #@backoff.on_exception(backoff.expo, openai.error.RateLimitError, on_backoff=lambda x: print(f\"\"\"Backing off: {round(x['wait'])} seconds\"\"\"), )  \n",
    "    #representation_model = Cohere(co, delay_in_seconds=12)\n",
    "    #representation_model = KeyBERTInspired(model='all-MiniLM-L6-v2')\n",
    "    tm = BERTopic(\n",
    "        embedding_model=embedding_model,          # Step 1 - Extract embeddings\n",
    "        umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n",
    "        hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n",
    "        vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n",
    "        ctfidf_model=ctfidf_model ,               # Step 5 - Extract topic words\n",
    "        #representation_model=representation_model # Step 6 - (Optional) Fine-tune topic represenations\n",
    "    )\n",
    "    topics, probs = tm.fit_transform(docs, embeddings)\n",
    "    df[\"Topic\"] = topics\n",
    "    df[\"probs\"] = probs\n",
    "\n",
    "    tm_df = tm.get_topic_info()\n",
    "    tm_df.set_index('Topic', inplace=True)\n",
    "    tm_df[\"Top_Words\"] = tm.get_topics()\n",
    "    tm_df.reset_index(inplace=True)\n",
    "    tm_df.Name = tm_df.Name.apply(lambda x: f\"T {x.split('_')[0]}: {', '.join([y.capitalize() for y in x.split('_')[1:]])}\" if int(\n",
    "        x.split('_')[0]) != -1 else \"No Topic\")\n",
    "    \n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    matching_dict = map_topics_risks_and_paragraphs(df, tm_df, df_risks, top_k, embedding_name)\n",
    "    save_path = base_path/'Dashboard/topic_models'/(embedding_name+f\"_{n_neigbors}\")\n",
    "    if not save_path.exists():\n",
    "        save_path.mkdir(parents=True)\n",
    "    for df_name, match_df in matching_dict.items():\n",
    "        match_df.to_pickle(save_path/f\"{df_name}.pkl\")\n",
    "\n",
    "    topics_2_risks = matching_dict[\"topics_to_risks\"].merge(df_risks, right_index=True, left_on='risks_index', how='left')\n",
    "    topic_risk_year_counts = topics_2_risks.groupby([\"Topic\", \"Year\"]).count()[[\"risks_index\"]].rename(columns={\"risks_index\": \"Count\"}).reset_index()\n",
    "\n",
    "    ## Only drop embeddings after computing similarites\n",
    "    df.drop(columns=[col for col in df.columns if col[:3] == \"emb\"], inplace=True)\n",
    "    topic_map = dict(zip(tm_df.Topic.tolist(), tm_df.Name.tolist()))\n",
    "    df['Topic_Name'] = df['Topic'].map(topic_map)\n",
    "    df['text_wrapped'] = df.text.apply(lambda x: \"<br>\".join(textwrap.wrap(x, width=60)))\n",
    "    df_risks.drop(columns=[col for col in df_risks.columns if col[:3] == \"emb\"], inplace=True)\n",
    "\n",
    "    df.rename(columns={\"year\": \"Year\"}, inplace=True)\n",
    "    year_counts = df.groupby([\"Topic\", \"Year\"]).count()[[\"report_id\"]].rename(columns={\"report_id\": \"Count\"}).reset_index()\n",
    "    tm_df[\"Year_Counts\"] = tm_df.Topic.apply(lambda x: get_year_counts(x, year_counts))\n",
    "    tm_df[\"Year_Counts_Risks\"] = tm_df.Topic.apply(lambda x: get_year_counts(x, topic_risk_year_counts))\n",
    "    tm_df[\"Max_Count_Year\"] = tm_df.Year_Counts.apply(lambda x: get_max_count_year(x))\n",
    "    tm_df[\"First_Year\"] = tm_df.Year_Counts.apply(lambda x: get_first_year(x))\n",
    "    tm_df[\"Last_Year\"] = tm_df.Year_Counts.apply(lambda x: get_last_year(x))\n",
    "    tm_df[\"Entropy\"] = tm_df.Year_Counts.apply(lambda x: get_entropy(x))\n",
    "    tm_df[\"Embeddings\"] = tm.topic_embeddings_.tolist()\n",
    "\n",
    "    hierarchy = tm.hierarchical_topics(docs)\n",
    "\n",
    "    df.to_pickle(save_path/'df.pkl')\n",
    "    df_risks.to_pickle(save_path/'df_risks.pkl')\n",
    "    tm_df.to_pickle(save_path/'tm_df.pkl')\n",
    "    hierarchy.to_pickle(save_path/'hierarchy.pkl')\n",
    "    return [df, tm_df, hierarchy, tm, matching_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:02<00:00, 93.67it/s] \n"
     ]
    }
   ],
   "source": [
    "results = create_and_save_topic_model_and_sematic_similarity(df.copy(), embedding_name=embedding_name, n_neigbors=n_neighbors, df_risks=df_risks.copy(), top_k=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
