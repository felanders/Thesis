{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import datasets\n",
    "from dotenv import dotenv_values\n",
    "from pathlib import Path\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np.random.seed(19950808)\n",
    "\n",
    "# take environment variables from .env.\n",
    "config = dotenv_values(\"./../../config/.env\")\n",
    "base_path = Path(config[\"BASE_PATH\"])\n",
    "writing_path = base_path/\"writing\"/\"MSc-Thesis-Emerging-Risks\"\n",
    "table_path = writing_path/\"tables\"\n",
    "sys.path.append(str(base_path/\"code\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KW Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_al = pd.read_pickle(base_path/\"data/labeling/active-learning-iteration-2.pkl\")\n",
    "df_al = df_al[df_al.labeled]\n",
    "loss = df_al.loss.astype(bool)\n",
    "unexpected = df_al.unexpected.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"\"\"\n",
    "    powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\n",
    "    \"\"\"\n",
    "    xs = list(iterable)\n",
    "    # note we return an iterator rather than a list\n",
    "    return chain.from_iterable(combinations(xs,n) for n in range(len(xs)+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the true labeled paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_al[df_al.loss == 1].text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_al[df_al.unexpected == 1].text.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_labels = [\n",
    "    \"loss\",\n",
    "    \"adverse development\",\n",
    "    \"adverse effect\",\n",
    "    \"adverse impact\",\n",
    "    \"adverse influence\"\n",
    "    \"higher claims\",\n",
    "    \"higher loss\",\n",
    "    \"higher cost\"\n",
    "    \"costs increased\",\n",
    "    \"impariment\",\n",
    "    \"charge\",\n",
    "    \"rising claims expenses\",\n",
    "    \"negatively impacted\",\n",
    "    \"burden\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_labels = list(powerset(loss_labels))\n",
    "l_scores = np.zeros((len(l_labels), 5))\n",
    "for i, labels in enumerate(l_labels):\n",
    "    kw = df_al.text.apply(lambda x: any([y.lower() in x.lower() for y in labels]))\n",
    "    l_scores[i] = (f1_score(loss, kw), fbeta_score(loss, kw, beta=2), precision_score(loss, kw, zero_division=0), recall_score(loss, kw), roc_auc_score(loss, kw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rising claims expenses\n",
      " \n",
      "F1: 0.004\n",
      "F2: 0.003\n",
      "TP: 1\n",
      "FP: 0\n",
      "FN: 450\n",
      "TN: 1049\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.70      1.00      0.82      1049\n",
      "        True       1.00      0.00      0.00       451\n",
      "\n",
      "    accuracy                           0.70      1500\n",
      "   macro avg       0.85      0.50      0.41      1500\n",
      "weighted avg       0.79      0.70      0.58      1500\n",
      "\n",
      "########################################\n",
      " \n",
      "loss, adverse development, adverse effect, charge, rising claims expenses, burden\n",
      " \n",
      "F1: 0.561\n",
      "F2: 0.66\n",
      "TP: 338\n",
      "FP: 417\n",
      "FN: 113\n",
      "TN: 632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.85      0.60      0.70      1049\n",
      "        True       0.45      0.75      0.56       451\n",
      "\n",
      "    accuracy                           0.65      1500\n",
      "   macro avg       0.65      0.68      0.63      1500\n",
      "weighted avg       0.73      0.65      0.66      1500\n",
      "\n",
      "########################################\n",
      " \n",
      "loss, adverse development, charge, rising claims expenses, burden\n",
      " \n",
      "F1: 0.567\n",
      "F2: 0.663\n",
      "TP: 337\n",
      "FP: 400\n",
      "FN: 114\n",
      "TN: 649\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.85      0.62      0.72      1049\n",
      "        True       0.46      0.75      0.57       451\n",
      "\n",
      "    accuracy                           0.66      1500\n",
      "   macro avg       0.65      0.68      0.64      1500\n",
      "weighted avg       0.73      0.66      0.67      1500\n",
      "\n",
      "########################################\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for index in set(l_scores.argmax(axis=0)):\n",
    "    print(\", \".join(l_labels[index]))\n",
    "    print(\" \")\n",
    "    kw = df_al.text.apply(lambda x: any([y.lower() in x.lower() for y in l_labels[index]]))\n",
    "    print(\"F1:\", round(f1_score(loss, kw), 3))\n",
    "    print(\"F2:\", round(fbeta_score(loss, kw, beta=2), 3))\n",
    "    conf_mat = confusion_matrix(loss, kw)\n",
    "    print(\"TP:\", conf_mat[1,1])\n",
    "    print(\"FP:\", conf_mat[0,1])\n",
    "    print(\"FN:\", conf_mat[1,0])\n",
    "    print(\"TN:\", conf_mat[0,0])\n",
    "    print(classification_report(loss, kw))\n",
    "    print(40*\"#\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_labels = [\n",
    "    \"loss\",\n",
    "    \"adverse development\",\n",
    "    \"charge\",\n",
    "    \"rising claims expenses\",\n",
    "    \"burden\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unexpected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_labels = [\n",
    "    \"unexpected\",\n",
    "    \"surprising\",\n",
    "    \"surprised\",\n",
    "    \"surpris\",\n",
    "    \"not expected\",\n",
    "    \"expected\",\n",
    "    \"more than expected\",\n",
    "    \"less than expected\",\n",
    "    \"lower than expected\"\n",
    "    \"higher than expected \"\n",
    "    \"more than expected\"\n",
    "    \"below expectations\",\n",
    "    \"above expectations\",\n",
    "    \"exceed expectations\",\n",
    "    \"exceeded expectations\",\n",
    "    \"not meet expectations\",\n",
    "    \"not according to expectations\",\n",
    "    \"not as expected\",\n",
    "    \"estimated\",\n",
    "    \"anticipated\",\n",
    "    \"predicted\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Subset\n",
    "This is too large to run all combinations thus i used different subsets, refinde iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_labels = [\n",
    "    \"unexpected\",\n",
    "    \"surprising\",\n",
    "    \"surprised\",\n",
    "    \"surpris\",\n",
    "    \"not expected\",\n",
    "    \"expected\",\n",
    "    \"more than expected\",\n",
    "    \"less than expected\",\n",
    "    \"lower than expected\"\n",
    "    \"higher than expected \"\n",
    "    \"more than expected\"\n",
    "    \"below expectations\",\n",
    "    \"above expectations\",\n",
    "    \"exceed expectations\",\n",
    "    \"exceeded expectations\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_labels = [\n",
    "    \"unexpect\",\n",
    "    \"expectation\"\n",
    "    \"surpris\",\n",
    "    \"expected\",\n",
    "    \"below expectations\",\n",
    "    \"above expectations\",\n",
    "    \"exceed expectations\",\n",
    "    \"exceeded expectations\",\n",
    "    \"estimated\",\n",
    "    \"anticipated\",\n",
    "    \"predicted\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_labels = [\n",
    "    \"expected\",\n",
    "    \"surprised\",\n",
    "    \"below expectations\",\n",
    "    \"above expectations\",\n",
    "    \"exceeded expectations\",\n",
    "    \"anticipated\",\n",
    "    \"predicted\",\n",
    "    \"not meet expectations\",\n",
    "    \"not according to expectations\",\n",
    "    \"not as expected\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_labels = list(powerset(unexpected_labels))\n",
    "u_scores = np.zeros((len(u_labels), 5))\n",
    "for i, labels in enumerate(u_labels):\n",
    "    kw = df_al.text.apply(lambda x: any([y.lower() in x.lower() for y in labels]))\n",
    "    u_scores[i] = (f1_score(unexpected, kw), fbeta_score(unexpected, kw, beta=2), precision_score(unexpected, kw, zero_division=0), recall_score(unexpected, kw), roc_auc_score(unexpected, kw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected, below expectations, above expectations, exceeded expectations, anticipated\n",
      " \n",
      "F1: 0.372\n",
      "F2: 0.474\n",
      "TP: 106\n",
      "FP: 281\n",
      "FN: 77\n",
      "TN: 1036\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.79      0.85      1317\n",
      "        True       0.27      0.58      0.37       183\n",
      "\n",
      "    accuracy                           0.76      1500\n",
      "   macro avg       0.60      0.68      0.61      1500\n",
      "weighted avg       0.85      0.76      0.79      1500\n",
      "\n",
      "########################################\n",
      " \n",
      "expected, below expectations, above expectations, exceeded expectations, anticipated, predicted\n",
      " \n",
      "F1: 0.37\n",
      "F2: 0.475\n",
      "TP: 107\n",
      "FP: 288\n",
      "FN: 76\n",
      "TN: 1029\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.78      0.85      1317\n",
      "        True       0.27      0.58      0.37       183\n",
      "\n",
      "    accuracy                           0.76      1500\n",
      "   macro avg       0.60      0.68      0.61      1500\n",
      "weighted avg       0.85      0.76      0.79      1500\n",
      "\n",
      "########################################\n",
      " \n",
      "above expectations\n",
      " \n",
      "F1: 0.011\n",
      "F2: 0.007\n",
      "TP: 1\n",
      "FP: 0\n",
      "FN: 182\n",
      "TN: 1317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      1.00      0.94      1317\n",
      "        True       1.00      0.01      0.01       183\n",
      "\n",
      "    accuracy                           0.88      1500\n",
      "   macro avg       0.94      0.50      0.47      1500\n",
      "weighted avg       0.89      0.88      0.82      1500\n",
      "\n",
      "########################################\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for index in set(u_scores.argmax(axis=0)):\n",
    "    print(\", \".join(u_labels[index]))\n",
    "    print(\" \")\n",
    "    kw = df_al.text.apply(lambda x: any([y.lower() in x.lower() for y in u_labels[index]]))\n",
    "    print(\"F1:\", round(f1_score(unexpected, kw), 3))\n",
    "    print(\"F2:\", round(fbeta_score(unexpected, kw, beta=2), 3))\n",
    "    conf_mat = confusion_matrix(unexpected, kw)\n",
    "    print(\"TP:\", conf_mat[1,1])\n",
    "    print(\"FP:\", conf_mat[0,1])\n",
    "    print(\"FN:\", conf_mat[1,0])\n",
    "    print(\"TN:\", conf_mat[0,0])\n",
    "    print(classification_report(unexpected, kw))\n",
    "    print(40*\"#\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_labels = [\n",
    "    \"expected\",\n",
    "    \"below expectations\",\n",
    "    \"above expectations\",\n",
    "    \"exceeded expectations\",\n",
    "    \"anticipated\",\n",
    "    \"predicted\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llm = pd.read_pickle(base_path/\"data/labeling/Eval-LLMs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_pickle(base_path/\"data/labeling/GT.pkl\")\n",
    "gt = gt[(gt.labeled == True) & (gt.strategy == \"sequential\")]\n",
    "gt_loss = gt.loss.astype(bool)\n",
    "gt_unexpected = gt.unexpected.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_loss = gt.text.apply(lambda x: any([y.lower() in x.lower() for y in loss_labels]))\n",
    "kw_unexpected = gt.text.apply(lambda x: any([y.lower() in x.lower() for y in unexpected_labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.96      0.98      2745\n",
      "        True       0.14      0.76      0.24        21\n",
      "\n",
      "    accuracy                           0.96      2766\n",
      "   macro avg       0.57      0.86      0.61      2766\n",
      "weighted avg       0.99      0.96      0.98      2766\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.84      0.90      2629\n",
      "        True       0.16      0.55      0.24       137\n",
      "\n",
      "    accuracy                           0.83      2766\n",
      "   macro avg       0.56      0.70      0.57      2766\n",
      "weighted avg       0.93      0.83      0.87      2766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(gt_unexpected, kw_unexpected))\n",
    "print(classification_report(gt_loss, kw_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.93      0.96      2745\n",
      "        True       0.01      0.10      0.02        21\n",
      "\n",
      "    accuracy                           0.92      2766\n",
      "   macro avg       0.50      0.51      0.49      2766\n",
      "weighted avg       0.99      0.92      0.95      2766\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.91      0.95      2629\n",
      "        True       0.28      0.64      0.39       137\n",
      "\n",
      "    accuracy                           0.90      2766\n",
      "   macro avg       0.63      0.77      0.67      2766\n",
      "weighted avg       0.94      0.90      0.92      2766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(gt_unexpected, df_llm.cohere_unexpected.apply(lambda x: x ==\"True\")))\n",
    "print(classification_report(gt_loss, df_llm.cohere_loss.apply(lambda x: x ==\"True\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred = {\n",
    "    \"loss\": {},\n",
    "    \"unexpected\": {},\n",
    "    \"unexpected_loss\": {}\n",
    "    }\n",
    "\n",
    "data_folder_path = base_path/\"data/evaluation\"\n",
    "for target in [\"loss\", \"unexpected\", \"unexpected_loss\"]:\n",
    "    dataset_path = data_folder_path/target\n",
    "    for dataset in os.listdir(dataset_path):\n",
    "        model =  \"-\".join(dataset.split(\"-\")[1:])\n",
    "        if model == \"ze\":\n",
    "            data = datasets.load_from_disk(dataset_path/dataset)\n",
    "            df = data.to_pandas()\n",
    "            df[\"p_ensemble\"] = df[[col for col in df.columns if col[:2] == \"p_\"]].mean(axis=1)\n",
    "            df = df[[\"report_id\", \"paragraph_nr\", \"p_ensemble\"]]\n",
    "        else:\n",
    "            data = datasets.load_from_disk(dataset_path/dataset)\n",
    "            df = data.to_pandas()\n",
    "            for col in df.columns:\n",
    "                if col[:6] == \"logits\":\n",
    "                    new_col = \"_\".join([\"p\"] + col.split(\"_\")[1:])\n",
    "                    df[new_col]= softmax(df[col].tolist(), axis=1)[:,0]\n",
    "                    df = df[[\"report_id\", \"paragraph_nr\"] + [new_col]]\n",
    "        for col in df.columns:\n",
    "            if col[:2] == \"p_\":\n",
    "                y_pred = (gt.merge(df, on=[\"report_id\", \"paragraph_nr\"], how='left')[col]>0.5).tolist()\n",
    "                model_pred[target][model] =  y_pred\n",
    "        if target == \"loss\":\n",
    "            model_pred[target][\"Keyword\"] = gt.text.apply(lambda x: any([y.lower() in x.lower() for y in loss_labels]))\n",
    "        elif target == \"unexpected\":\n",
    "            model_pred[target][\"Keyword\"] = gt.text.apply(lambda x: any([y.lower() in x.lower() for y in unexpected_labels]))\n",
    "    if target != \"unexpected_loss\":\n",
    "        model_pred[target][\"cohere\"] = df_llm[f\"cohere_{target}\"].apply(lambda x: x == \"True\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\n",
    "    \"loss\": {},\n",
    "    \"unexpected\": {},\n",
    "    \"unexpected_loss\": {}\n",
    "    }\n",
    "\n",
    "for target in [\"loss\", \"unexpected\", \"unexpected_loss\"]:\n",
    "    if target == \"unexpected_loss\":\n",
    "        y_gt = (gt_unexpected & gt_loss).tolist()\n",
    "        for key in model_pred[\"loss\"]:\n",
    "            y_pred = [x&y for x, y in zip(model_pred[\"loss\"][key], model_pred[\"unexpected\"][key])]\n",
    "            conf_mat = confusion_matrix(y_gt, y_pred)\n",
    "            scores[target][key] = {\n",
    "                \"F1\": f1_score(y_gt, y_pred, average=\"binary\"),\n",
    "                \"Precision\": precision_score(y_gt, y_pred, average=\"binary\", zero_division=0),\n",
    "                \"Recall\": recall_score(y_gt, y_pred, average=\"binary\"),\n",
    "                \"F2\": fbeta_score(y_gt, y_pred, beta=2, average=\"binary\"),\n",
    "                \"AUC-ROC\": roc_auc_score(y_gt, y_pred),\n",
    "                \"TP\": conf_mat[1,1],\n",
    "                \"FP\": conf_mat[0,1],\n",
    "                \"FN\": conf_mat[1,0],\n",
    "                \"TN\": conf_mat[0,0]\n",
    "            }\n",
    "        for key in model_pred[\"unexpected_loss\"]:\n",
    "            y_pred = model_pred[target][key]\n",
    "            conf_mat = confusion_matrix(y_gt, y_pred)\n",
    "            scores[target][key] = {\n",
    "                \"F1\": f1_score(y_gt, y_pred, average=\"binary\"),\n",
    "                \"Precision\": precision_score(y_gt, y_pred, average=\"binary\", zero_division=0),\n",
    "                \"Recall\": recall_score(y_gt, y_pred, average=\"binary\"),\n",
    "                \"F2\": fbeta_score(y_gt, y_pred, beta=2, average=\"binary\"),\n",
    "                \"AUC-ROC\": roc_auc_score(y_gt, y_pred),\n",
    "                \"TP\": conf_mat[1,1],\n",
    "                \"FP\": conf_mat[0,1],\n",
    "                \"FN\": conf_mat[1,0],\n",
    "                \"TN\": conf_mat[0,0]\n",
    "            }\n",
    "    else:\n",
    "        y_gt = gt[target].tolist()\n",
    "        for key in model_pred[target]:\n",
    "            y_pred = model_pred[target][key]\n",
    "            conf_mat = confusion_matrix(y_gt, y_pred)\n",
    "            scores[target][key] = {\n",
    "                \"F1\": f1_score(y_gt, y_pred, average=\"binary\"),\n",
    "                \"Precision\": precision_score(y_gt, y_pred, average=\"binary\", zero_division=0),\n",
    "                \"Recall\": recall_score(y_gt, y_pred, average=\"binary\"),\n",
    "                \"F2\": fbeta_score(y_gt, y_pred, beta=2, average=\"binary\"),\n",
    "                \"AUC-ROC\": roc_auc_score(y_gt, y_pred),\n",
    "                \"TP\": conf_mat[1,1],\n",
    "                \"FP\": conf_mat[0,1],\n",
    "                \"FN\": conf_mat[1,0],\n",
    "                \"TN\": conf_mat[0,0]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_map = {\n",
    "    \"Keyword\": \"Keyword\",\n",
    "    \"zediac-large\": \"ZeDiAc-large\",\n",
    "    \"zediac-base\": \"ZeDiAc-base\",\n",
    "    \"zedi-large\": \"ZeDi-large\",\n",
    "    \"zedi-base\": \"ZeDi-base\",\n",
    "    \"ze\": \"Zero-Shot\",\n",
    "    \"ft-large\": \"DeBERTa-large\",\n",
    "    \"ft-base\": \"DeBERTa-base\",\n",
    "    \"cohere\": \"Cohere\"\n",
    "}\n",
    "model_order = [key for key in model_map.keys()]\n",
    "score_order= [\"F1\", \"F2\", \"AUC-ROC\", \"Precision\", \"Recall\", \"TP\", \"FP\", \"FN\", \"TN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unexpected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores[\"unexpected\"])[model_order].rename(columns=model_map).T.round(2)[score_order].style.highlight_max(props=\"font-weight:bold;\", axis=0, subset=[\"F1\", \"F2\", \"AUC-ROC\", \"Precision\", \"Recall\", \"TP\", \"TN\"]).highlight_min(props=\"font-weight:bold;\", axis=0, subset=[\"FP\", \"FN\"]).format(\"{:.2f}\", subset=[\"F1\", \"F2\", \"AUC-ROC\", \"Precision\", \"Recall\"]).format(\"{:.0f}\", [\"TP\", \"TN\", \"FP\", \"FN\"]).to_latex(\n",
    "    table_path/\"model_eval_unexpected.tex\",\n",
    "    position=\"H\",\n",
    "    caption=\"Model performance for \\emph{unexpected}.\",\n",
    "    label=\"tab:model_eval_unexpected\",\n",
    "    environment=\"longtable\",\n",
    "    convert_css=True,\n",
    "    hrules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores[\"loss\"])[model_order].rename(columns=model_map).T.round(2)[score_order].style.highlight_max(props=\"font-weight:bold;\", axis=0, subset=[\"F1\", \"F2\", \"AUC-ROC\", \"Precision\", \"Recall\", \"TP\", \"TN\"]).highlight_min(props=\"font-weight:bold;\", axis=0, subset=[\"FP\", \"FN\"]).format(\"{:.2f}\", subset=[\"F1\", \"F2\", \"AUC-ROC\", \"Precision\", \"Recall\"]).format(\"{:.0f}\", [\"TP\", \"TN\", \"FP\", \"FN\"]).to_latex(\n",
    "    table_path/\"model_eval_loss.tex\",\n",
    "    position=\"H\",\n",
    "    caption=\"Model performance for \\emph{loss}.\",\n",
    "    label=\"tab:model_eval_loss\",\n",
    "    environment=\"longtable\",\n",
    "    convert_css=True,\n",
    "    hrules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unexpected Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores[\"unexpected_loss\"]).rename(columns=model_map).T.round(2)[score_order].style.highlight_max(props=\"font-weight:bold;\", axis=0, subset=[\"F1\", \"F2\", \"AUC-ROC\", \"Precision\", \"Recall\", \"TP\", \"TN\"]).highlight_min(props=\"font-weight:bold;\", axis=0, subset=[\"FP\", \"FN\"]).format(\"{:.2f}\", subset=[\"F1\", \"F2\", \"AUC-ROC\", \"Precision\", \"Recall\"]).format(\"{:.0f}\", [\"TP\", \"TN\", \"FP\", \"FN\"]).to_latex(\n",
    "    table_path/\"model_eval_unexpected_loss.tex\",\n",
    "    position=\"H\",\n",
    "    caption=\"Model performance for \\emph{unexpected \\& loss}.\",\n",
    "    label=\"tab:model_eval_unexpected_loss\",\n",
    "    environment=\"longtable\",\n",
    "    convert_css=True,\n",
    "    hrules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Zero-Shot performance of different Models, Targets, and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_zs = {\n",
    "    \"loss\": {},\n",
    "    \"unexpected\": {},\n",
    "    \"unexpected_loss\": {}\n",
    "    }\n",
    "for target in [\"loss\", \"unexpected\"]:\n",
    "    scores_zs[target][\"Ensemble\"] = scores[target][\"ze\"]\n",
    "    data = datasets.load_from_disk(data_folder_path/target/\"eval-ze\")\n",
    "    df = data.to_pandas()\n",
    "    df = gt.merge(df[[col for col in df.columns if col[:2] == \"p_\"]+[\"report_id\", \"paragraph_nr\"]], on =[\"report_id\", \"paragraph_nr\"], how=\"left\")\n",
    "\n",
    "    y_pred = {}\n",
    "    for col in [col for col in df.columns if col[:2] == \"p_\"]:\n",
    "        y_pred[col] = (df[col]>0.5).to_list()\n",
    "\n",
    "    y_gt = df[target].to_list()\n",
    "    for col in y_pred:\n",
    "        y = y_pred[col]\n",
    "        model = \"RoBERTa\" if col.split(\"_\")[2] == \"D\" else \"DeBERTa\"\n",
    "        key = \"-\".join([model, col.split(\"_\")[3], col.split(\"_\")[4]])\n",
    "        conf_mat = confusion_matrix(y_gt, y)\n",
    "        scores_zs[target][key] = {\n",
    "                \"F1\": f1_score(y_gt, y, average=\"binary\"),\n",
    "                \"Precision\": precision_score(y_gt, y, average=\"binary\", zero_division=0),\n",
    "                \"Recall\": recall_score(y_gt, y, average=\"binary\"),\n",
    "                \"F2\": fbeta_score(y_gt, y, beta=2, average=\"binary\"),\n",
    "                \"AUC-ROC\": roc_auc_score(y_gt, y),\n",
    "                \"TP\": conf_mat[1,1],\n",
    "                \"FP\": conf_mat[0,1],\n",
    "                \"FN\": conf_mat[1,0],\n",
    "                \"TN\": conf_mat[0,0]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores_zs[\"unexpected\"]).rename(columns=model_map).T.round(2)[score_order].style.highlight_max(props=\"font-weight:bold;\", axis=0, subset=[\"F1\", \"F2\", \"AUC-ROC\", \"Precision\", \"Recall\", \"TP\", \"TN\"]).highlight_min(props=\"font-weight:bold;\", axis=0, subset=[\"FP\", \"FN\"]).format(\"{:.2f}\", subset=[\"F1\", \"F2\", \"AUC-ROC\", \"Precision\", \"Recall\"]).format(\"{:.0f}\", [\"TP\", \"TN\", \"FP\", \"FN\"]).to_latex(\n",
    "    table_path/\"model_eval_zero_shot_unexpected.tex\",\n",
    "    position=\"H\",\n",
    "    caption=\"Zero-shot model performance for \\emph{unexpected}.\",\n",
    "    label=\"tab:model_eval_zero_shot_unexpected\",\n",
    "    environment=\"longtable\",\n",
    "    convert_css=True,\n",
    "    hrules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores_zs[\"loss\"]).rename(columns=model_map).T.round(2)[score_order].style.highlight_max(props=\"font-weight:bold;\", axis=0, subset=[\"F1\", \"F2\", \"AUC-ROC\", \"Precision\", \"Recall\", \"TP\", \"TN\"]).highlight_min(props=\"font-weight:bold;\", axis=0, subset=[\"FP\", \"FN\"]).format(\"{:.2f}\", subset=[\"F1\", \"F2\", \"AUC-ROC\", \"Precision\", \"Recall\"]).format(\"{:.0f}\", [\"TP\", \"TN\", \"FP\", \"FN\"]).to_latex(\n",
    "    table_path/\"model_eval_zero_shot_loss.tex\",\n",
    "    position=\"H\",\n",
    "    caption=\"Zero-shot model performance for \\emph{loss}.\",\n",
    "    label=\"tab:model_eval_zero_shot_loss\",\n",
    "    environment=\"longtable\",\n",
    "    convert_css=True,\n",
    "    hrules=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
