{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import datasets\n",
    "from dotenv import dotenv_values\n",
    "from pathlib import Path\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import PrecisionRecallDisplay, classification_report, f1_score, fbeta_score, precision_score, accuracy_score, recall_score, confusion_matrix, RocCurveDisplay, roc_auc_score, ConfusionMatrixDisplay\n",
    "np.random.seed(19950808)\n",
    "\n",
    "# take environment variables from .env.\n",
    "config = dotenv_values(\"./../../config/.env\")\n",
    "base_path = Path(config[\"BASE_PATH\"])\n",
    "writing_path = base_path/\"writing\"/\"MSc-Thesis-Emerging-Risks\"\n",
    "table_path = writing_path/\"tables\"\n",
    "sys.path.append(str(base_path/\"code\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KW Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_al = pd.read_pickle(base_path/\"data/labeling/active-learning-iteration-2.pkl\")\n",
    "df_al = df_al[df_al.labeled]\n",
    "loss = df_al.loss.astype(bool)\n",
    "unexpected = df_al.unexpected.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"\"\"\n",
    "    powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\n",
    "    \"\"\"\n",
    "    xs = list(iterable)\n",
    "    # note we return an iterator rather than a list\n",
    "    return chain.from_iterable(combinations(xs,n) for n in range(len(xs)+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the true labeled paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_al[df_al.loss == 1].text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_al[df_al.unexpected == 1].text.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_labels = [\n",
    "    \"loss\",\n",
    "    \"adverse development\",\n",
    "    \"adverse effect\",\n",
    "    \"adverse impact\",\n",
    "    \"adverse influence\"\n",
    "    \"higher claims\",\n",
    "    \"higher loss\",\n",
    "    \"higher cost\"\n",
    "    \"costs increased\",\n",
    "    \"impariment\",\n",
    "    \"charge\",\n",
    "    \"rising claims expenses\",\n",
    "    \"negatively impacted\",\n",
    "    \"burden\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/andreas/Polybox/Project-Support-Material/Thesis/notebooks/Evaluation/Evaluation.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andreas/Polybox/Project-Support-Material/Thesis/notebooks/Evaluation/Evaluation.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, labels \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(l_labels):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andreas/Polybox/Project-Support-Material/Thesis/notebooks/Evaluation/Evaluation.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     kw \u001b[39m=\u001b[39m df_al\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39many\u001b[39m([y\u001b[39m.\u001b[39mlower() \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m labels]))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/andreas/Polybox/Project-Support-Material/Thesis/notebooks/Evaluation/Evaluation.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     l_scores[i] \u001b[39m=\u001b[39m (f1_score(loss, kw), fbeta_score(loss, kw, beta\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m), precision_score(loss, kw, zero_division\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), recall_score(loss, kw), roc_auc_score(loss, kw))\n",
      "File \u001b[0;32m~/Polybox/Project-Support-Material/Thesis/.venv/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:572\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    570\u001b[0m     labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y_true)\n\u001b[1;32m    571\u001b[0m     y_true \u001b[39m=\u001b[39m label_binarize(y_true, classes\u001b[39m=\u001b[39mlabels)[:, \u001b[39m0\u001b[39m]\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mreturn\u001b[39;00m _average_binary_score(\n\u001b[1;32m    573\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[39m=\u001b[39;49mmax_fpr),\n\u001b[1;32m    574\u001b[0m         y_true,\n\u001b[1;32m    575\u001b[0m         y_score,\n\u001b[1;32m    576\u001b[0m         average,\n\u001b[1;32m    577\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    578\u001b[0m     )\n\u001b[1;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# multilabel-indicator\u001b[39;00m\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m _average_binary_score(\n\u001b[1;32m    581\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[39m=\u001b[39mmax_fpr),\n\u001b[1;32m    582\u001b[0m         y_true,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    585\u001b[0m         sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[1;32m    586\u001b[0m     )\n",
      "File \u001b[0;32m~/Polybox/Project-Support-Material/Thesis/.venv/lib/python3.10/site-packages/sklearn/metrics/_base.py:70\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m average \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m average_options:\n\u001b[1;32m     68\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39maverage has to be one of \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(average_options))\n\u001b[0;32m---> 70\u001b[0m y_type \u001b[39m=\u001b[39m type_of_target(y_true)\n\u001b[1;32m     71\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmultilabel-indicator\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     72\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m format is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n",
      "File \u001b[0;32m~/Polybox/Project-Support-Material/Thesis/.venv/lib/python3.10/site-packages/sklearn/utils/multiclass.py:386\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39m# Check multiclass\u001b[39;00m\n\u001b[1;32m    385\u001b[0m first_row \u001b[39m=\u001b[39m y[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issparse(y) \u001b[39melse\u001b[39;00m y\u001b[39m.\u001b[39mgetrow(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mdata\n\u001b[0;32m--> 386\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39;49munique_values(y)\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m (y\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(first_row) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    387\u001b[0m     \u001b[39m# [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\u001b[39;00m\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m suffix\n\u001b[1;32m    389\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Polybox/Project-Support-Material/Thesis/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:84\u001b[0m, in \u001b[0;36m_NumPyApiWrapper.unique_values\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munique_values\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 84\u001b[0m     \u001b[39mreturn\u001b[39;00m numpy\u001b[39m.\u001b[39;49munique(x)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Polybox/Project-Support-Material/Thesis/.venv/lib/python3.10/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[39m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[1;32m    275\u001b[0m                     equal_nan\u001b[39m=\u001b[39;49mequal_nan)\n\u001b[1;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[39m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m~/Polybox/Project-Support-Material/Thesis/.venv/lib/python3.10/site-packages/numpy/lib/arraysetops.py:355\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    352\u001b[0m     mask[\u001b[39m1\u001b[39m:] \u001b[39m=\u001b[39m aux[\u001b[39m1\u001b[39m:] \u001b[39m!=\u001b[39m aux[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    354\u001b[0m ret \u001b[39m=\u001b[39m (aux[mask],)\n\u001b[0;32m--> 355\u001b[0m \u001b[39mif\u001b[39;00m return_index:\n\u001b[1;32m    356\u001b[0m     ret \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (perm[mask],)\n\u001b[1;32m    357\u001b[0m \u001b[39mif\u001b[39;00m return_inverse:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "l_labels = list(powerset(loss_labels))\n",
    "l_scores = np.zeros((len(l_labels), 5))\n",
    "for i, labels in enumerate(l_labels):\n",
    "    kw = df_al.text.apply(lambda x: any([y.lower() in x.lower() for y in labels]))\n",
    "    l_scores[i] = (f1_score(loss, kw), fbeta_score(loss, kw, beta=2), precision_score(loss, kw, zero_division=0), recall_score(loss, kw), roc_auc_score(loss, kw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rising claims expenses\n",
      " \n",
      "F1: 0.004\n",
      "F2: 0.003\n",
      "TP: 1\n",
      "FP: 0\n",
      "FN: 450\n",
      "TN: 1049\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.70      1.00      0.82      1049\n",
      "        True       1.00      0.00      0.00       451\n",
      "\n",
      "    accuracy                           0.70      1500\n",
      "   macro avg       0.85      0.50      0.41      1500\n",
      "weighted avg       0.79      0.70      0.58      1500\n",
      "\n",
      "########################################\n",
      " \n",
      "loss, adverse development, adverse effect, charge, rising claims expenses, burden\n",
      " \n",
      "F1: 0.561\n",
      "F2: 0.66\n",
      "TP: 338\n",
      "FP: 417\n",
      "FN: 113\n",
      "TN: 632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.85      0.60      0.70      1049\n",
      "        True       0.45      0.75      0.56       451\n",
      "\n",
      "    accuracy                           0.65      1500\n",
      "   macro avg       0.65      0.68      0.63      1500\n",
      "weighted avg       0.73      0.65      0.66      1500\n",
      "\n",
      "########################################\n",
      " \n",
      "loss, adverse development, charge, rising claims expenses, burden\n",
      " \n",
      "F1: 0.567\n",
      "F2: 0.663\n",
      "TP: 337\n",
      "FP: 400\n",
      "FN: 114\n",
      "TN: 649\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.85      0.62      0.72      1049\n",
      "        True       0.46      0.75      0.57       451\n",
      "\n",
      "    accuracy                           0.66      1500\n",
      "   macro avg       0.65      0.68      0.64      1500\n",
      "weighted avg       0.73      0.66      0.67      1500\n",
      "\n",
      "########################################\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for index in set(l_scores.argmax(axis=0)):\n",
    "    print(\", \".join(l_labels[index]))\n",
    "    print(\" \")\n",
    "    kw = df_al.text.apply(lambda x: any([y.lower() in x.lower() for y in l_labels[index]]))\n",
    "    print(\"F1:\", round(f1_score(loss, kw), 3))\n",
    "    print(\"F2:\", round(fbeta_score(loss, kw, beta=2), 3))\n",
    "    conf_mat = confusion_matrix(loss, kw)\n",
    "    print(\"TP:\", conf_mat[1,1])\n",
    "    print(\"FP:\", conf_mat[0,1])\n",
    "    print(\"FN:\", conf_mat[1,0])\n",
    "    print(\"TN:\", conf_mat[0,0])\n",
    "    print(classification_report(loss, kw))\n",
    "    print(40*\"#\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_labels = [\n",
    "    \"loss\",\n",
    "    \"adverse development\",\n",
    "    \"charge\",\n",
    "    \"rising claims expenses\",\n",
    "    \"burden\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unexpected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_labels = [\n",
    "    \"unexpected\",\n",
    "    \"surprising\",\n",
    "    \"surprised\",\n",
    "    \"surpris\",\n",
    "    \"not expected\",\n",
    "    \"expected\",\n",
    "    \"more than expected\",\n",
    "    \"less than expected\",\n",
    "    \"lower than expected\"\n",
    "    \"higher than expected \"\n",
    "    \"more than expected\"\n",
    "    \"below expectations\",\n",
    "    \"above expectations\",\n",
    "    \"exceed expectations\",\n",
    "    \"exceeded expectations\",\n",
    "    \"not meet expectations\",\n",
    "    \"not according to expectations\",\n",
    "    \"not as expected\",\n",
    "    \"estimated\",\n",
    "    \"anticipated\",\n",
    "    \"predicted\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Subset\n",
    "This is too large to run all combinations thus i used different subsets, refinde iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_labels = [\n",
    "    \"unexpected\",\n",
    "    \"surprising\",\n",
    "    \"surprised\",\n",
    "    \"surpris\",\n",
    "    \"not expected\",\n",
    "    \"expected\",\n",
    "    \"more than expected\",\n",
    "    \"less than expected\",\n",
    "    \"lower than expected\"\n",
    "    \"higher than expected \"\n",
    "    \"more than expected\"\n",
    "    \"below expectations\",\n",
    "    \"above expectations\",\n",
    "    \"exceed expectations\",\n",
    "    \"exceeded expectations\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_labels = [\n",
    "    \"unexpect\",\n",
    "    \"expectation\"\n",
    "    \"surpris\",\n",
    "    \"expected\",\n",
    "    \"below expectations\",\n",
    "    \"above expectations\",\n",
    "    \"exceed expectations\",\n",
    "    \"exceeded expectations\",\n",
    "    \"estimated\",\n",
    "    \"anticipated\",\n",
    "    \"predicted\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_labels = [\n",
    "    \"expected\",\n",
    "    \"surprised\",\n",
    "    \"below expectations\",\n",
    "    \"above expectations\",\n",
    "    \"exceeded expectations\",\n",
    "    \"anticipated\",\n",
    "    \"predicted\",\n",
    "    \"not meet expectations\",\n",
    "    \"not according to expectations\",\n",
    "    \"not as expected\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_labels = list(powerset(unexpected_labels))\n",
    "u_scores = np.zeros((len(u_labels), 5))\n",
    "for i, labels in enumerate(u_labels):\n",
    "    kw = df_al.text.apply(lambda x: any([y.lower() in x.lower() for y in labels]))\n",
    "    u_scores[i] = (f1_score(unexpected, kw), fbeta_score(unexpected, kw, beta=2), precision_score(unexpected, kw, zero_division=0), recall_score(unexpected, kw), roc_auc_score(unexpected, kw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected, below expectations, above expectations, exceeded expectations, anticipated\n",
      " \n",
      "F1: 0.372\n",
      "F2: 0.474\n",
      "TP: 106\n",
      "FP: 281\n",
      "FN: 77\n",
      "TN: 1036\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.79      0.85      1317\n",
      "        True       0.27      0.58      0.37       183\n",
      "\n",
      "    accuracy                           0.76      1500\n",
      "   macro avg       0.60      0.68      0.61      1500\n",
      "weighted avg       0.85      0.76      0.79      1500\n",
      "\n",
      "########################################\n",
      " \n",
      "expected, below expectations, above expectations, exceeded expectations, anticipated, predicted\n",
      " \n",
      "F1: 0.37\n",
      "F2: 0.475\n",
      "TP: 107\n",
      "FP: 288\n",
      "FN: 76\n",
      "TN: 1029\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.78      0.85      1317\n",
      "        True       0.27      0.58      0.37       183\n",
      "\n",
      "    accuracy                           0.76      1500\n",
      "   macro avg       0.60      0.68      0.61      1500\n",
      "weighted avg       0.85      0.76      0.79      1500\n",
      "\n",
      "########################################\n",
      " \n",
      "above expectations\n",
      " \n",
      "F1: 0.011\n",
      "F2: 0.007\n",
      "TP: 1\n",
      "FP: 0\n",
      "FN: 182\n",
      "TN: 1317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      1.00      0.94      1317\n",
      "        True       1.00      0.01      0.01       183\n",
      "\n",
      "    accuracy                           0.88      1500\n",
      "   macro avg       0.94      0.50      0.47      1500\n",
      "weighted avg       0.89      0.88      0.82      1500\n",
      "\n",
      "########################################\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for index in set(u_scores.argmax(axis=0)):\n",
    "    print(\", \".join(u_labels[index]))\n",
    "    print(\" \")\n",
    "    kw = df_al.text.apply(lambda x: any([y.lower() in x.lower() for y in u_labels[index]]))\n",
    "    print(\"F1:\", round(f1_score(unexpected, kw), 3))\n",
    "    print(\"F2:\", round(fbeta_score(unexpected, kw, beta=2), 3))\n",
    "    conf_mat = confusion_matrix(unexpected, kw)\n",
    "    print(\"TP:\", conf_mat[1,1])\n",
    "    print(\"FP:\", conf_mat[0,1])\n",
    "    print(\"FN:\", conf_mat[1,0])\n",
    "    print(\"TN:\", conf_mat[0,0])\n",
    "    print(classification_report(unexpected, kw))\n",
    "    print(40*\"#\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_labels = [\n",
    "    \"expected\",\n",
    "    \"below expectations\",\n",
    "    \"above expectations\",\n",
    "    \"exceeded expectations\",\n",
    "    \"anticipated\",\n",
    "    \"predicted\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llm = pd.read_pickle(base_path/\"data/labeling/Eval-LLMs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_pickle(base_path/\"data/labeling/GT.pkl\")\n",
    "gt = gt[(gt.labeled == True) & (gt.strategy == \"sequential\")]\n",
    "gt_loss = gt.loss.astype(bool)\n",
    "gt_unexpected = gt.unexpected.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_loss = gt.text.apply(lambda x: any([y.lower() in x.lower() for y in loss_labels]))\n",
    "kw_unexpected = gt.text.apply(lambda x: any([y.lower() in x.lower() for y in unexpected_labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.96      0.98      2745\n",
      "        True       0.14      0.76      0.24        21\n",
      "\n",
      "    accuracy                           0.96      2766\n",
      "   macro avg       0.57      0.86      0.61      2766\n",
      "weighted avg       0.99      0.96      0.98      2766\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.84      0.90      2629\n",
      "        True       0.16      0.55      0.24       137\n",
      "\n",
      "    accuracy                           0.83      2766\n",
      "   macro avg       0.56      0.70      0.57      2766\n",
      "weighted avg       0.93      0.83      0.87      2766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(gt_unexpected, kw_unexpected))\n",
    "print(classification_report(gt_loss, kw_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.93      0.96      2745\n",
      "        True       0.01      0.10      0.02        21\n",
      "\n",
      "    accuracy                           0.92      2766\n",
      "   macro avg       0.50      0.51      0.49      2766\n",
      "weighted avg       0.99      0.92      0.95      2766\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.91      0.95      2629\n",
      "        True       0.28      0.64      0.39       137\n",
      "\n",
      "    accuracy                           0.90      2766\n",
      "   macro avg       0.63      0.77      0.67      2766\n",
      "weighted avg       0.94      0.90      0.92      2766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(gt_unexpected, df_llm.cohere_unexpected.apply(lambda x: x ==\"True\")))\n",
    "print(classification_report(gt_loss, df_llm.cohere_loss.apply(lambda x: x ==\"True\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred = {\n",
    "    \"loss\": {},\n",
    "    \"unexpected\": {},\n",
    "    \"unexpected_loss\": {}\n",
    "    }\n",
    "\n",
    "data_folder_path = base_path/\"data/evaluation\"\n",
    "for target in [\"loss\", \"unexpected\", \"unexpected_loss\"]:\n",
    "    dataset_path = data_folder_path/target\n",
    "    for dataset in os.listdir(dataset_path):\n",
    "        model =  \"-\".join(dataset.split(\"-\")[1:])\n",
    "        if model == \"ze\":\n",
    "            data = datasets.load_from_disk(dataset_path/dataset)\n",
    "            df = data.to_pandas()\n",
    "            df[\"p_ensemble\"] = df[[col for col in df.columns if col[:2] == \"p_\"]].mean(axis=1)\n",
    "            df = df[[\"report_id\", \"paragraph_nr\", \"p_ensemble\"]]\n",
    "        else:\n",
    "            data = datasets.load_from_disk(dataset_path/dataset)\n",
    "            df = data.to_pandas()\n",
    "            for col in df.columns:\n",
    "                if col[:6] == \"logits\":\n",
    "                    new_col = \"_\".join([\"p\"] + col.split(\"_\")[1:])\n",
    "                    df[new_col]= softmax(df[col].tolist(), axis=1)[:,0]\n",
    "                    df = df[[\"report_id\", \"paragraph_nr\"] + [new_col]]\n",
    "        for col in df.columns:\n",
    "            if col[:2] == \"p_\":\n",
    "                y_pred = (gt.merge(df, on=[\"report_id\", \"paragraph_nr\"], how='left')[col]>0.5).tolist()\n",
    "                model_pred[target][model] =  y_pred\n",
    "        if target == \"loss\":\n",
    "            model_pred[target][\"Keyword\"] = gt.text.apply(lambda x: any([y.lower() in x.lower() for y in loss_labels]))\n",
    "        elif target == \"unexpected\":\n",
    "            model_pred[target][\"Keyword\"] = gt.text.apply(lambda x: any([y.lower() in x.lower() for y in unexpected_labels]))\n",
    "    if target != \"unexpected_loss\":\n",
    "        model_pred[target][\"cohere\"] = df_llm[f\"cohere_{target}\"].apply(lambda x: x == \"True\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\n",
    "    \"loss\": {},\n",
    "    \"unexpected\": {},\n",
    "    \"unexpected_loss\": {}\n",
    "    }\n",
    "\n",
    "for target in [\"loss\", \"unexpected\", \"unexpected_loss\"]:\n",
    "    if target == \"unexpected_loss\":\n",
    "        y_gt = (gt_unexpected & gt_loss).tolist()\n",
    "        for key in model_pred[\"loss\"]:\n",
    "            y_pred = [x&y for x, y in zip(model_pred[\"loss\"][key], model_pred[\"unexpected\"][key])]\n",
    "            conf_mat = confusion_matrix(y_gt, y_pred)\n",
    "            scores[target][key] = {\n",
    "                \"F1\": f1_score(y_gt, y_pred, average=\"binary\"),\n",
    "                \"Precision\": precision_score(y_gt, y_pred, average=\"binary\", zero_division=0),\n",
    "                \"Recall\": recall_score(y_gt, y_pred, average=\"binary\"),\n",
    "                \"F2\": fbeta_score(y_gt, y_pred, beta=2, average=\"binary\"),\n",
    "                \"Accuracy\": accuracy_score(y_gt, y_pred),\n",
    "                \"TP\": conf_mat[1,1],\n",
    "                \"FP\": conf_mat[0,1],\n",
    "                \"FN\": conf_mat[1,0],\n",
    "                \"TN\": conf_mat[0,0]\n",
    "            }\n",
    "        for key in model_pred[\"unexpected_loss\"]:\n",
    "            y_pred = model_pred[target][key]\n",
    "            conf_mat = confusion_matrix(y_gt, y_pred)\n",
    "            scores[target][key] = {\n",
    "                \"F1\": f1_score(y_gt, y_pred, average=\"binary\"),\n",
    "                \"Precision\": precision_score(y_gt, y_pred, average=\"binary\", zero_division=0),\n",
    "                \"Recall\": recall_score(y_gt, y_pred, average=\"binary\"),\n",
    "                \"F2\": fbeta_score(y_gt, y_pred, beta=2, average=\"binary\"),\n",
    "                \"Accuracy\": accuracy_score(y_gt, y_pred),\n",
    "                \"TP\": conf_mat[1,1],\n",
    "                \"FP\": conf_mat[0,1],\n",
    "                \"FN\": conf_mat[1,0],\n",
    "                \"TN\": conf_mat[0,0]\n",
    "            }\n",
    "    else:\n",
    "        y_gt = gt[target].tolist()\n",
    "        for key in model_pred[target]:\n",
    "            y_pred = model_pred[target][key]\n",
    "            conf_mat = confusion_matrix(y_gt, y_pred)\n",
    "            scores[target][key] = {\n",
    "                \"F1\": f1_score(y_gt, y_pred, average=\"binary\"),\n",
    "                \"Precision\": precision_score(y_gt, y_pred, average=\"binary\", zero_division=0),\n",
    "                \"Recall\": recall_score(y_gt, y_pred, average=\"binary\"),\n",
    "                \"F2\": fbeta_score(y_gt, y_pred, beta=2, average=\"binary\"),\n",
    "                \"Accuracy\": accuracy_score(y_gt, y_pred),\n",
    "                \"TP\": conf_mat[1,1],\n",
    "                \"FP\": conf_mat[0,1],\n",
    "                \"FN\": conf_mat[1,0],\n",
    "                \"TN\": conf_mat[0,0]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_map = {\n",
    "    \"Keyword\": \"Keyword\",\n",
    "    \"zediac-large\": \"ZeDiAc-large\",\n",
    "    \"zediac-base\": \"ZeDiAc-base\",\n",
    "    \"zedi-large\": \"ZeDi-large\",\n",
    "    \"zedi-base\": \"ZeDi-base\",\n",
    "    \"ze\": \"Zero-Shot\",\n",
    "    \"ft-large\": \"DeBERTa-large\",\n",
    "    \"ft-base\": \"DeBERTa-base\",\n",
    "    \"cohere\": \"Cohere\"\n",
    "}\n",
    "model_order = [key for key in model_map.keys()]\n",
    "score_order= [\"F1\", \"F2\", \"Accuracy\", \"Precision\", \"Recall\", \"TP\", \"FP\", \"FN\", \"TN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unexpected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores[\"unexpected\"])[model_order].rename(columns=model_map).T.round(2)[score_order].style.highlight_max(props=\"font-weight:bold;\", axis=0, subset=[\"F1\", \"F2\",\"Accuracy\", \"Precision\", \"Recall\", \"TP\", \"TN\"]).highlight_min(props=\"font-weight:bold;\", axis=0, subset=[\"FP\", \"FN\"]).format(\"{:.2f}\", subset=[\"F1\", \"F2\", \"Accuracy\",\"Precision\", \"Recall\"]).format(\"{:.0f}\", [\"TP\", \"TN\", \"FP\", \"FN\"]).to_latex(\n",
    "    table_path/\"model_eval_unexpected.tex\",\n",
    "    position=\"H\",\n",
    "    caption=\"Model performance for \\emph{unexpected}.\",\n",
    "    label=\"tab:model_eval_unexpected\",\n",
    "    environment=\"longtable\",\n",
    "    convert_css=True,\n",
    "    hrules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores[\"loss\"])[model_order].rename(columns=model_map).T.round(2)[score_order].style.highlight_max(props=\"font-weight:bold;\", axis=0, subset=[\"F1\", \"F2\", \"Accuracy\", \"Precision\", \"Recall\", \"TP\", \"TN\"]).highlight_min(props=\"font-weight:bold;\", axis=0, subset=[\"FP\", \"FN\"]).format(\"{:.2f}\", subset=[\"F1\", \"F2\", \"Accuracy\",\"Precision\", \"Recall\"]).format(\"{:.0f}\", [\"TP\", \"TN\", \"FP\", \"FN\"]).to_latex(\n",
    "    table_path/\"model_eval_loss.tex\",\n",
    "    position=\"H\",\n",
    "    caption=\"Model performance for \\emph{loss}.\",\n",
    "    label=\"tab:model_eval_loss\",\n",
    "    environment=\"longtable\",\n",
    "    convert_css=True,\n",
    "    hrules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unexpected Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores[\"unexpected_loss\"]).rename(columns=model_map).T.round(2)[score_order].style.highlight_max(props=\"font-weight:bold;\", axis=0, subset=[\"F1\", \"F2\", \"Accuracy\", \"Precision\", \"Recall\", \"TP\", \"TN\"]).highlight_min(props=\"font-weight:bold;\", axis=0, subset=[\"FP\", \"FN\"]).format(\"{:.2f}\", subset=[\"F1\", \"F2\",\"Accuracy\", \"Precision\", \"Recall\"]).format(\"{:.0f}\", [\"TP\", \"TN\", \"FP\", \"FN\"]).to_latex(\n",
    "    table_path/\"model_eval_unexpected_loss.tex\",\n",
    "    position=\"H\",\n",
    "    caption=\"Model performance for \\emph{unexpected \\& loss}.\",\n",
    "    label=\"tab:model_eval_unexpected_loss\",\n",
    "    environment=\"longtable\",\n",
    "    convert_css=True,\n",
    "    hrules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Zero-Shot performance of different Models, Targets, and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_zs = {\n",
    "    \"loss\": {},\n",
    "    \"unexpected\": {},\n",
    "    \"unexpected_loss\": {}\n",
    "    }\n",
    "for target in [\"loss\", \"unexpected\"]:\n",
    "    scores_zs[target][\"Ensemble\"] = scores[target][\"ze\"]\n",
    "    data = datasets.load_from_disk(data_folder_path/target/\"eval-ze\")\n",
    "    df = data.to_pandas()\n",
    "    df = gt.merge(df[[col for col in df.columns if col[:2] == \"p_\"]+[\"report_id\", \"paragraph_nr\"]], on =[\"report_id\", \"paragraph_nr\"], how=\"left\")\n",
    "\n",
    "    y_pred = {}\n",
    "    for col in [col for col in df.columns if col[:2] == \"p_\"]:\n",
    "        y_pred[col] = (df[col]>0.5).to_list()\n",
    "\n",
    "    y_gt = df[target].to_list()\n",
    "    for col in y_pred:\n",
    "        y = y_pred[col]\n",
    "        model = \"RoBERTa\" if col.split(\"_\")[2] == \"D\" else \"DeBERTa\"\n",
    "        key = \"-\".join([model, col.split(\"_\")[3], col.split(\"_\")[4]])\n",
    "        conf_mat = confusion_matrix(y_gt, y)\n",
    "        scores_zs[target][key] = {\n",
    "                \"F1\": f1_score(y_gt, y, average=\"binary\"),\n",
    "                \"Precision\": precision_score(y_gt, y, average=\"binary\", zero_division=0),\n",
    "                \"Recall\": recall_score(y_gt, y, average=\"binary\"),\n",
    "                \"F2\": fbeta_score(y_gt, y, beta=2, average=\"binary\"),\n",
    "                \"Accuracy\": accuracy_score(y_gt, y),\n",
    "                \"TP\": conf_mat[1,1],\n",
    "                \"FP\": conf_mat[0,1],\n",
    "                \"FN\": conf_mat[1,0],\n",
    "                \"TN\": conf_mat[0,0]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores_zs[\"unexpected\"]).rename(columns=model_map).T.round(2)[score_order].style.highlight_max(props=\"font-weight:bold;\", axis=0, subset=[\"F1\", \"F2\", \"Accuracy\", \"Precision\", \"Recall\", \"TP\", \"TN\"]).highlight_min(props=\"font-weight:bold;\", axis=0, subset=[\"FP\", \"FN\"]).format(\"{:.2f}\", subset=[\"F1\", \"F2\", \"Accuracy\",  \"Precision\", \"Recall\"]).format(\"{:.0f}\", [\"TP\", \"TN\", \"FP\", \"FN\"]).to_latex(\n",
    "    table_path/\"model_eval_zero_shot_unexpected.tex\",\n",
    "    position=\"H\",\n",
    "    caption=\"Zero-shot model performance for \\emph{unexpected}.\",\n",
    "    label=\"tab:model_eval_zero_shot_unexpected\",\n",
    "    environment=\"longtable\",\n",
    "    convert_css=True,\n",
    "    hrules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores_zs[\"loss\"]).rename(columns=model_map).T.round(2)[score_order].style.highlight_max(props=\"font-weight:bold;\", axis=0, subset=[\"F1\", \"F2\", \"Accuracy\", \"Precision\", \"Recall\", \"TP\", \"TN\"]).highlight_min(props=\"font-weight:bold;\", axis=0, subset=[\"FP\", \"FN\"]).format(\"{:.2f}\", subset=[\"F1\", \"F2\", \"Accuracy\", \"Precision\", \"Recall\"]).format(\"{:.0f}\", [\"TP\", \"TN\", \"FP\", \"FN\"]).to_latex(\n",
    "    table_path/\"model_eval_zero_shot_loss.tex\",\n",
    "    position=\"H\",\n",
    "    caption=\"Zero-shot model performance for \\emph{loss}.\",\n",
    "    label=\"tab:model_eval_zero_shot_loss\",\n",
    "    environment=\"longtable\",\n",
    "    convert_css=True,\n",
    "    hrules=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
