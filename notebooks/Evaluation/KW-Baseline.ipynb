{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dotenv import dotenv_values\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score, fbeta_score, precision_score, recall_score, confusion_matrix\n",
    "np.random.seed(19950808)\n",
    "\n",
    "# take environment variables from .env.\n",
    "config = dotenv_values(\"./../../config/.env\")\n",
    "base_path = Path(config[\"BASE_PATH\"])\n",
    "writing_path = base_path/\"writing\"/\"MSc-Thesis-Emerging-Risks\"\n",
    "table_path = writing_path/\"tables\"\n",
    "sys.path.append(str(base_path/\"code\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"\"\"\n",
    "    powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\n",
    "    \"\"\"\n",
    "    xs = list(iterable)\n",
    "    # note we return an iterator rather than a list\n",
    "    return chain.from_iterable(combinations(xs,n) for n in range(len(xs)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_al = pd.read_pickle(base_path/\"data/labeling/active-learning-iteration-2.pkl\")\n",
    "df_al = df_al[df_al.labeled]\n",
    "loss = df_al.loss.astype(bool)\n",
    "unexpected = df_al.unexpected.astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the true labeled paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_al[df_al.loss == 1].text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_al[df_al.unexpected == 1].text.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_labels = [\n",
    "    \"loss\",\n",
    "    \"adverse development\",\n",
    "    \"adverse effect\",\n",
    "    \"adverse impact\",\n",
    "    \"adverse influence\"\n",
    "    \"higher claims\",\n",
    "    \"higher loss\",\n",
    "    \"higher cost\"\n",
    "    \"costs increased\",\n",
    "    \"impariment\",\n",
    "    \"charge\",\n",
    "    \"rising claims expenses\",\n",
    "    \"negatively impacted\",\n",
    "    \"burden\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_labels = list(powerset(loss_labels))\n",
    "l_scores = np.zeros((len(l_labels), 4))\n",
    "for i, labels in enumerate(l_labels):\n",
    "    kw = df_al.text.apply(lambda x: any([y.lower() in x.lower() for y in labels]))\n",
    "    l_scores[i] = (f1_score(loss, kw), fbeta_score(loss, kw, beta=2), precision_score(loss, kw, zero_division=0), recall_score(loss, kw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rising claims expenses\n",
      " \n",
      "F1: 0.004\n",
      "F2: 0.003\n",
      "TP: 1\n",
      "FP: 0\n",
      "FN: 450\n",
      "TN: 1049\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.70      1.00      0.82      1049\n",
      "        True       1.00      0.00      0.00       451\n",
      "\n",
      "    accuracy                           0.70      1500\n",
      "   macro avg       0.85      0.50      0.41      1500\n",
      "weighted avg       0.79      0.70      0.58      1500\n",
      "\n",
      "########################################\n",
      " \n",
      "loss, adverse development, adverse effect, charge, rising claims expenses, burden\n",
      " \n",
      "F1: 0.561\n",
      "F2: 0.66\n",
      "TP: 338\n",
      "FP: 417\n",
      "FN: 113\n",
      "TN: 632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.85      0.60      0.70      1049\n",
      "        True       0.45      0.75      0.56       451\n",
      "\n",
      "    accuracy                           0.65      1500\n",
      "   macro avg       0.65      0.68      0.63      1500\n",
      "weighted avg       0.73      0.65      0.66      1500\n",
      "\n",
      "########################################\n",
      " \n",
      "loss, adverse development, charge, rising claims expenses, burden\n",
      " \n",
      "F1: 0.567\n",
      "F2: 0.663\n",
      "TP: 337\n",
      "FP: 400\n",
      "FN: 114\n",
      "TN: 649\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.85      0.62      0.72      1049\n",
      "        True       0.46      0.75      0.57       451\n",
      "\n",
      "    accuracy                           0.66      1500\n",
      "   macro avg       0.65      0.68      0.64      1500\n",
      "weighted avg       0.73      0.66      0.67      1500\n",
      "\n",
      "########################################\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for index in set(l_scores.argmax(axis=0)):\n",
    "    print(\", \".join(l_labels[index]))\n",
    "    print(\" \")\n",
    "    kw = df_al.text.apply(lambda x: any([y.lower() in x.lower() for y in l_labels[index]]))\n",
    "    print(\"F1:\", round(f1_score(loss, kw), 3))\n",
    "    print(\"F2:\", round(fbeta_score(loss, kw, beta=2), 3))\n",
    "    conf_mat = confusion_matrix(loss, kw)\n",
    "    print(\"TP:\", conf_mat[1,1])\n",
    "    print(\"FP:\", conf_mat[0,1])\n",
    "    print(\"FN:\", conf_mat[1,0])\n",
    "    print(\"TN:\", conf_mat[0,0])\n",
    "    print(classification_report(loss, kw))\n",
    "    print(40*\"#\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_labels = [\n",
    "    \"loss\",\n",
    "    \"adverse development\",\n",
    "    \"charge\",\n",
    "    \"rising claims expenses\",\n",
    "    \"burden\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unexpected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_labels = [\n",
    "    \"unexpected\",\n",
    "    \"surprising\",\n",
    "    \"surprised\",\n",
    "    \"surpris\",\n",
    "    \"not expected\",\n",
    "    \"expected\",\n",
    "    \"more than expected\",\n",
    "    \"less than expected\",\n",
    "    \"lower than expected\"\n",
    "    \"higher than expected \"\n",
    "    \"more than expected\"\n",
    "    \"below expectations\",\n",
    "    \"above expectations\",\n",
    "    \"exceed expectations\",\n",
    "    \"exceeded expectations\",\n",
    "    \"not meet expectations\",\n",
    "    \"not according to expectations\",\n",
    "    \"not as expected\",\n",
    "    \"estimated\",\n",
    "    \"anticipated\",\n",
    "    \"predicted\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Subset\n",
    "This is too large to run all combinations thus i used different subsets, refinde iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_labels = [\n",
    "    \"unexpected\",\n",
    "    \"surprising\",\n",
    "    \"surprised\",\n",
    "    \"surpris\",\n",
    "    \"not expected\",\n",
    "    \"expected\",\n",
    "    \"more than expected\",\n",
    "    \"less than expected\",\n",
    "    \"lower than expected\"\n",
    "    \"higher than expected \"\n",
    "    \"more than expected\"\n",
    "    \"below expectations\",\n",
    "    \"above expectations\",\n",
    "    \"exceed expectations\",\n",
    "    \"exceeded expectations\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_labels = [\n",
    "    \"unexpect\",\n",
    "    \"expectation\"\n",
    "    \"surpris\",\n",
    "    \"expected\",\n",
    "    \"below expectations\",\n",
    "    \"above expectations\",\n",
    "    \"exceed expectations\",\n",
    "    \"exceeded expectations\",\n",
    "    \"estimated\",\n",
    "    \"anticipated\",\n",
    "    \"predicted\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_labels = [\n",
    "    \"expected\",\n",
    "    \"surprised\",\n",
    "    \"below expectations\",\n",
    "    \"above expectations\",\n",
    "    \"exceeded expectations\",\n",
    "    \"anticipated\",\n",
    "    \"predicted\",\n",
    "    \"not meet expectations\",\n",
    "    \"not according to expectations\",\n",
    "    \"not as expected\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_labels = list(powerset(unexpected_labels))\n",
    "u_scores = np.zeros((len(u_labels), 4))\n",
    "for i, labels in enumerate(u_labels):\n",
    "    kw = df_al.text.apply(lambda x: any([y.lower() in x.lower() for y in labels]))\n",
    "    u_scores[i] = (f1_score(unexpected, kw), fbeta_score(unexpected, kw, beta=2), precision_score(unexpected, kw, zero_division=0), recall_score(unexpected, kw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected, below expectations, above expectations, exceeded expectations, anticipated\n",
      " \n",
      "F1: 0.372\n",
      "F2: 0.474\n",
      "TP: 106\n",
      "FP: 281\n",
      "FN: 77\n",
      "TN: 1036\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.79      0.85      1317\n",
      "        True       0.27      0.58      0.37       183\n",
      "\n",
      "    accuracy                           0.76      1500\n",
      "   macro avg       0.60      0.68      0.61      1500\n",
      "weighted avg       0.85      0.76      0.79      1500\n",
      "\n",
      "########################################\n",
      " \n",
      "expected, below expectations, above expectations, exceeded expectations, anticipated, predicted\n",
      " \n",
      "F1: 0.37\n",
      "F2: 0.475\n",
      "TP: 107\n",
      "FP: 288\n",
      "FN: 76\n",
      "TN: 1029\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.78      0.85      1317\n",
      "        True       0.27      0.58      0.37       183\n",
      "\n",
      "    accuracy                           0.76      1500\n",
      "   macro avg       0.60      0.68      0.61      1500\n",
      "weighted avg       0.85      0.76      0.79      1500\n",
      "\n",
      "########################################\n",
      " \n",
      "above expectations\n",
      " \n",
      "F1: 0.011\n",
      "F2: 0.007\n",
      "TP: 1\n",
      "FP: 0\n",
      "FN: 182\n",
      "TN: 1317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      1.00      0.94      1317\n",
      "        True       1.00      0.01      0.01       183\n",
      "\n",
      "    accuracy                           0.88      1500\n",
      "   macro avg       0.94      0.50      0.47      1500\n",
      "weighted avg       0.89      0.88      0.82      1500\n",
      "\n",
      "########################################\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for index in set(u_scores.argmax(axis=0)):\n",
    "    print(\", \".join(u_labels[index]))\n",
    "    print(\" \")\n",
    "    kw = df_al.text.apply(lambda x: any([y.lower() in x.lower() for y in u_labels[index]]))\n",
    "    print(\"F1:\", round(f1_score(unexpected, kw), 3))\n",
    "    print(\"F2:\", round(fbeta_score(unexpected, kw, beta=2), 3))\n",
    "    conf_mat = confusion_matrix(unexpected, kw)\n",
    "    print(\"TP:\", conf_mat[1,1])\n",
    "    print(\"FP:\", conf_mat[0,1])\n",
    "    print(\"FN:\", conf_mat[1,0])\n",
    "    print(\"TN:\", conf_mat[0,0])\n",
    "    print(classification_report(unexpected, kw))\n",
    "    print(40*\"#\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpected_labels = [\n",
    "    \"expected\",\n",
    "    \"below expectations\",\n",
    "    \"above expectations\",\n",
    "    \"exceeded expectations\",\n",
    "    \"anticipated\",\n",
    "    \"predicted\"\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
