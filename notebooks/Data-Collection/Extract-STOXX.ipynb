{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is adapted from https://github.com/dafrie/fin-disclosures-nlp and to be run with the dependencies referenced there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import dotenv_values\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "config = dotenv_values(\"./../../config/.env\") # take environment variables from .env.\n",
    "base_path = Path(config[\"BASE_PATH\"])\n",
    "sys.path.append(str(base_path/\"modules\"))\n",
    "data_path = base_path/\"data\"\n",
    "writing_path = base_path/\"writing\"/\"MSc-Thesis-Emerging-Risks\"\n",
    "table_path = writing_path/\"tables\"\n",
    "figure_path = writing_path/\"figures\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get path information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_paths(path):\n",
    "    result = []\n",
    "    for f in os.scandir(path):\n",
    "        if not f.name.startswith('.') and f.is_dir():\n",
    "            result.append(f)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_reports_paths(path):\n",
    "    result = []\n",
    "    for f in os.scandir(path):\n",
    "        p = Path(f.path)\n",
    "        if not f.name.startswith('.') and p.suffix == '.pdf' and f.is_file():\n",
    "            result.append(p)\n",
    "    return result\n",
    "\n",
    "def get_reports_paths_information():\n",
    "    regex= \"\\d{4}\"\n",
    "    rows = []\n",
    "    column_names = ['report_id', 'company', 'year', 'path', 'extracted', 'inferred']\n",
    "    company_paths = get_company_paths(data_path/\"stoxx\"/\"annual_reports_raw\")\n",
    "    for company_dir in company_paths:\n",
    "        company_files = get_reports_paths(company_dir.path)\n",
    "        company = company_dir.name\n",
    "        if os.path.exists(data_path/\"stoxx\"/\"annual_reports_extracted\"/company):\n",
    "            extracted_years = [x.split(\".\")[0] for x in os.listdir(data_path/\"stoxx\"/\"annual_reports_extracted\"/company) if x[-4:]==\".yml\"]\n",
    "        else:\n",
    "            extracted_years = []\n",
    "        for p in company_files:\n",
    "            matches = re.findall(\"\\d{4}\", p.stem)\n",
    "            try:\n",
    "                assert len(matches) == 1, f\"Less / more than one year matched!: {p}\"\n",
    "                year = matches[0]\n",
    "                report_id = f\"{company}-AR_{year}\"\n",
    "                if year in extracted_years:\n",
    "                    rows.append([report_id, company, year, p, True, False ])\n",
    "                else:\n",
    "                    rows.append([report_id, company, year, p, False, False ])\n",
    "            except:\n",
    "                print(f\"Error - Invalid reports file found: {p}\")\n",
    "                break;\n",
    "                                                    \n",
    "    df_reports = pd.DataFrame(rows, columns=column_names)\n",
    "    df_reports.set_index(\"report_id\", inplace=True)\n",
    "    return df_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path/\"stoxx\"/\"stoxx_europe_600_financials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex = df.rename(columns={\"company_name\": \"Company\"})\n",
    "latex.Insurance = df.Insurance.apply(lambda x: \"Insurance\" if x.lower() == \"true\" else ( \"Non-Insurance\" if x.lower() == \"false\" else \"Partly-Insurance\"))\n",
    "s = latex[[\"Company\", \"Insurance\"]].style.hide()\n",
    "s.to_latex(table_path/\"STOXX_Insurance_labling.tex\", column_format=\"ll\", label=\"tab:stoxx_insurance_labels\", caption=\"Insurance labelling of STOXX Europe 600 Financials companies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reports = get_reports_paths_information()\n",
    "df = pd.read_csv(data_path/\"stoxx\"/\"stoxx_europe_600_financials.csv\")\n",
    "\n",
    "insurance_company_folders = df[df[\"Insurance\"].apply(lambda x: x.lower() == \"true\")][\"company_folder\"].unique()\n",
    "insurance_company_folders = np.concatenate((insurance_company_folders, np.array([\"SwissReCorporateSolutions\"])))  #Provided by Swiss Re but not part of STOXX 600\n",
    "partly_insurance_company_folders = df[df[\"Insurance\"].apply(lambda x: x.lower() == \"partly\")][\"company_folder\"].unique()\n",
    "financials_company_folders = df[df[\"Insurance\"].apply(lambda x: x.lower() == \"false\")][\"company_folder\"].unique()\n",
    "\n",
    "df_insurance = df_reports[df_reports[\"company\"].isin(np.concatenate((insurance_company_folders, partly_insurance_company_folders)))][[\"company\", \"year\"]]\n",
    "df_insurance[\"only_insurance\"] = df_insurance[\"company\"].isin(insurance_company_folders)\n",
    "df_insurance[\"filing_type\"] = \"annual_report\"\n",
    "df_insurance.to_pickle(data_path/\"stoxx\"/\"master_stoxx.pkl\")\n",
    "\n",
    "df_financials = df_reports[df_reports[\"company\"].isin(financials_company_folders)][[\"company\", \"year\"]]\n",
    "df_financials[\"only_insurance\"] = False\n",
    "df_financials[\"filing_type\"] = \"annual_report\"\n",
    "df_financials.to_pickle(data_path/\"stoxx\"/\"financials_main.pkl\")\n",
    "\n",
    "df_reports.drop([\"year\", \"company\"], axis = 1, inplace=True)\n",
    "df_reports.to_csv(data_path/\"stoxx\"/\"extraction_master.csv\", index=False)\n",
    "df_reports.to_pickle(data_path/\"stoxx\"/\"extraction_master.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fin_disclosure_nlp.pdf_extractor import PdfExtractor\n",
    "from fin_disclosure_nlp.preprocessing import DocumentPreprocessor\n",
    "from tqdm.notebook  import tqdm\n",
    "from multiprocessing import Pool\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import yaml\n",
    "executor = ProcessPoolExecutor(max_workers=4)\n",
    "\n",
    "df_insurance = pd.read_pickle(data_path/\"stoxx\"/\"master_stoxx.pkl\")\n",
    "df_reports = pd.read_pickle(data_path/\"stoxx\"/\"extraction_master.pkl\")\n",
    "df = df_insurance.join(df_reports, on=\"report_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insurance = pd.read_pickle(data_path/\"stoxx\"/\"master_stoxx.pkl\")\n",
    "df_reports = pd.read_pickle(data_path/\"stoxx\"/\"extraction_master.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "report_id\n",
       "AegonNV-AR_2003    /home/andreas/Polybox/Project-Support-Material...\n",
       "Name: path, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"extracted\"] == False][\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Tasks in the queue\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f77b6fa51f4dcea18ffbe9059e0bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_file(input_file, output_folder, **kwargs):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    PdfExtractor(input_file=input_file, output_folder=output_folder, **kwargs)\n",
    "    \n",
    "futures = []\n",
    "\n",
    "for index, row in df[df[\"extracted\"] == False].iterrows():\n",
    "    out_folder = str(Path(row.path).parent).replace(\"_raw\", \"_extracted\")\n",
    "    futures.append(executor.submit(extract_file, row.path, out_folder))\n",
    "\n",
    "print(\"All Tasks in the queue\")\n",
    "\n",
    "for future in tqdm(futures, total=len(futures)):\n",
    "    try:\n",
    "        future.result()\n",
    "    except Exception as e:\n",
    "        print(f\"Error\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(data_path/\"stoxx\"/\"paragraphs_stoxx.pkl\"):\n",
    "    df_stoxx = pd.read_pickle(data_path/\"stoxx\"/\"paragraphs_stoxx.pkl\")\n",
    "else:\n",
    "    df_stoxx = pd.DataFrame()\n",
    "\n",
    "df_reports = get_reports_paths_information().drop([\"company\", \"year\"], axis=1)\n",
    "df = df_insurance[~df_insurance.index.isin(df_stoxx.reset_index()[\"report_id\"].unique())].join(df_reports, on=\"report_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Tasks in the queue\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0207b9b0c510497e936391838209662f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_file(input_file, **kwargs):\n",
    "    with open(str(input_file).replace(\"_raw\", \"_extracted\").replace('.pdf', '.yml'), 'r') as f:\n",
    "        pages = yaml.safe_load(f)\n",
    "    paragraphs = []\n",
    "    page_numbers = []\n",
    "    for i, page in enumerate(pages[\"pages\"]):\n",
    "        p = DocumentPreprocessor(page[\"text\"]).process().split(\"\\n\\n\")\n",
    "        paragraphs += p\n",
    "        page_numbers += [page[\"page_no\"]] * len(p)\n",
    "    year = [Path(input_file).stem] * len(paragraphs)\n",
    "    company = [f\"{Path(input_file).parent.stem}-AR_{Path(input_file).stem}\"] * len(paragraphs)\n",
    "    df_inter = pd.DataFrame({\"report_id\": company, \"year\": year, \"page_no\": page_numbers, \"text\": paragraphs})\n",
    "    df_inter = df_inter[df_inter[\"text\"].str.len() > 0]\n",
    "    return df_inter\n",
    "\n",
    "futures = []\n",
    "paths = []\n",
    "for index, row in df[df.extracted].iterrows():\n",
    "    futures.append(executor.submit(process_file, row.path))\n",
    "    paths.append(row.path)\n",
    "print(\"All Tasks in the queue\")\n",
    "\n",
    "for future, path in tqdm(zip(futures, paths), total=len(futures)):\n",
    "    try:\n",
    "        df_inter = future.result()\n",
    "        df_stoxx = pd.concat([df_stoxx, df_inter], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error\")\n",
    "        print(e)\n",
    "        print(path)\n",
    "        os.remove(str(path).replace(\"_raw\", \"_extracted\").replace('.pdf', '.yml'))\n",
    "\n",
    "df_stoxx.to_pickle(data_path/\"stoxx\"/\"paragraphs_stoxx.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stoxx = pd.read_pickle(data_path/\"paragraphs_stoxx.pkl\")\n",
    "df_stoxx.drop([\"year\"], axis=1, inplace=True)\n",
    "df_stoxx.text = df_stoxx.text.str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "df_stoxx[\"n_words\"] = df_stoxx.text.str.split(r\"\\s\", regex=True).str.len()\n",
    "df_stoxx[\"paragraph_nr\"] = df_stoxx.groupby([\"report_id\", \"page_no\"]).cumcount()\n",
    "df_stoxx[\"loss_kw\"] = df_stoxx.text.str.contains(\"loss\", case=False)\n",
    "df_stoxx[\"unexpected_kw\"] = df_stoxx.text.str.contains(\"unexpected\", case=False)\n",
    "df_stoxx.reset_index(inplace=True)  \n",
    "df_stoxx.n_words = pd.to_numeric(df_stoxx.n_words)\n",
    "df_stoxx.page_no = pd.to_numeric(df_stoxx.page_no)\n",
    "df_stoxx.paragraph_nr = pd.to_numeric(df_stoxx.paragraph_nr)\n",
    "df_stoxx.to_pickle(data_path/\"stoxx\"/\"paragraphs_stoxx.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_kw  unexpected_kw\n",
       "False    False            2267080\n",
       "         True                1043\n",
       "True     False             145825\n",
       "         True                 769\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stoxx.groupby([\"loss_kw\", \"unexpected_kw\"]).size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
